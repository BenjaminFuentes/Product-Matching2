

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>deepmatcher.models.modules &mdash; DeepMatcher 0.1.0rc1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> DeepMatcher
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../deepmatcher.html">deepmatcher</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../deepmatcher.html#main-modules">Main Modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../deepmatcher.html#matchingmodel"><span class="hidden-section">MatchingModel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepmatcher.html#attrsummarizer"><span class="hidden-section">AttrSummarizer</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepmatcher.html#classifier"><span class="hidden-section">Classifier</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../deepmatcher.html#components-of-attribute-summarizer">Components of Attribute Summarizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../deepmatcher.html#wordcontextualizer"><span class="hidden-section">WordContextualizer</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepmatcher.html#wordcomparator"><span class="hidden-section">WordComparator</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../deepmatcher.html#wordaggregator"><span class="hidden-section">WordAggregator</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../attr_summarizers.html">deepmatcher.attr_summarizers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../attr_summarizers.html#sif"><span class="hidden-section">SIF</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../attr_summarizers.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../attr_summarizers.html#attention"><span class="hidden-section">Attention</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../attr_summarizers.html#hybrid"><span class="hidden-section">Hybrid</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../word_contextualizers.html">deepmatcher.word_contextualizers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../word_contextualizers.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../word_contextualizers.html#selfattention"><span class="hidden-section">SelfAttention</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../word_comparators.html">deepmatcher.word_comparators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../word_aggregators.html">deepmatcher.word_aggregators</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../word_aggregators.html#pool"><span class="hidden-section">Pool</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../word_aggregators.html#attentionwithrnn"><span class="hidden-section">AttentionWithRNN</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../modules.html">deepmatcher.modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../modules.html#standard-operations">Standard Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#transform">Transform</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#pool">Pool</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#merge">Merge</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#align">Align</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#bypass">Bypass</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#rnn">RNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../modules.html#utility-modules">Utility Modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#lambda"><span class="hidden-section">Lambda</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#multisequential"><span class="hidden-section">MultiSequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#nometa"><span class="hidden-section">NoMeta</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#modulemap"><span class="hidden-section">ModuleMap</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#lazymodule"><span class="hidden-section">LazyModule</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../modules.html#lazymodulefn"><span class="hidden-section">LazyModuleFn</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">deepmatcher.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#softnllloss"><span class="hidden-section">SoftNLLLoss</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../optim.html#optimizer"><span class="hidden-section">Optimizer</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">deepmatcher.data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../data.html#process"><span class="hidden-section">process</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../data.html#process-unlabeled"><span class="hidden-section">process_unlabeled</span></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../batch.html">deepmatcher.batch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../batch.html#attrtensor"><span class="hidden-section">AttrTensor</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../batch.html#matchingbatch"><span class="hidden-section">MatchingBatch</span></a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">DeepMatcher</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>deepmatcher.models.modules</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for deepmatcher.models.modules</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="nn">abc</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">six</span>

<span class="kn">import</span> <span class="nn">deepmatcher</span> <span class="k">as</span> <span class="nn">dm</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="k">import</span> <span class="n">_utils</span>
<span class="kn">from</span> <span class="nn">..batch</span> <span class="k">import</span> <span class="n">AttrTensor</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;deepmatcher.modules&#39;</span><span class="p">)</span>


<div class="viewcode-block" id="LazyModule"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.LazyModule">[docs]</a><span class="nd">@six</span><span class="o">.</span><span class="n">add_metaclass</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABCMeta</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">LazyModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A lazily initialized module. Base class for most DeepMatcher modules.</span>

<span class="sd">    This module is an extension of PyTorch :class:`~torch.nn.Module` with the following</span>
<span class="sd">    property: constructing an instance this module does not immediately initialize it.</span>
<span class="sd">    This means that if the module has parameters, they will not be instantiated</span>
<span class="sd">    immediately after construction. The module is initialized the first time `forward` is</span>
<span class="sd">    called. This has the following benefits:</span>

<span class="sd">    * Can be safely deep copied to create structural clones that do not share</span>
<span class="sd">      parameters. E.g. deep copying a :class:`LazyModule` consisting of a 2 layer Linear</span>
<span class="sd">      NN will produce another :class:`LazyModule` with 2 layer Linear NN that 1) do not</span>
<span class="sd">      share parameters and 2) have different weight initializations.</span>
<span class="sd">    * Allows automatic input size inference. Refer to description of `_init` for details.</span>

<span class="sd">    This module also implements some additional goodies:</span>

<span class="sd">    * Output shape verification: As part of initialization, this module verifies that</span>
<span class="sd">      all output tensors have correct output shapes, if the expected output shape is</span>
<span class="sd">      specified using :meth:`expect_signature`. This verification is done only once during</span>
<span class="sd">      initialization to avoid slowing down training.</span>
<span class="sd">    * NaN checks: All module outputs are cheked for the presence of NaN values that may be</span>
<span class="sd">      difficult to trace down otherwise.</span>

<span class="sd">    Subclasses of this module are expected to override the following two methods:</span>

<span class="sd">    * _init(): This is where the constructor of the module should be defined. During the</span>
<span class="sd">      first forward pass, this method will be called to initialize the module. Whatever</span>
<span class="sd">      you typically define in the __init__ function of a PyTorch module, you may define</span>
<span class="sd">      it here. This function may optionally take in an `input_size` parameter. If it does,</span>
<span class="sd">      :class:`LazyModule` will set it to the size of the last dimension of the input.</span>
<span class="sd">      E.g., if the input is of size `32 * 300`, the `input_size` will be set to 300.</span>
<span class="sd">      Subclasses may choose not to override this method.</span>
<span class="sd">    * _forward(): This is where the computation for the forward pass of the module must be</span>
<span class="sd">      defined. Whatever you typically define in the forward function of a PyTorch module,</span>
<span class="sd">      you may define it here. All subclasses must override this method.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="LazyModule.__init__"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.LazyModule.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct a :class:`LazyModule`. DO NOT OVERRIDE this method.</span>

<span class="sd">        This does NOT initialize the module - construction simply saves the positional and</span>
<span class="sd">        keyword arguments for future initialization.</span>

<span class="sd">        Args:</span>
<span class="sd">            *args: Positional arguments to the constructor of the module defined in</span>
<span class="sd">                :meth:`_init`.</span>
<span class="sd">            **kwargs: Keyword arguments to the constructor of the module defined in</span>
<span class="sd">                :meth:`_init`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="LazyModule.forward"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.LazyModule.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Perform a forward pass through the module. DO NOT OVERRIDE this method.</span>

<span class="sd">        If the module is not initialized yet, this method also performs initialization.</span>
<span class="sd">        Initialization involves the following:</span>

<span class="sd">        1. Calling the :meth:`_init` method. Tries calling with the `input_size` keyword</span>
<span class="sd">           parameter set, along with the positional and keyword args specified during</span>
<span class="sd">           construction). If this fails with a :exc:`TypeError` (i.e., the</span>
<span class="sd">           :meth:`_init` method does not have an `input_size` parameter), then retries</span>
<span class="sd">           initialization without setting `input_size`.</span>
<span class="sd">        2. Verifying the output shape, if :meth:`expect_signature` was called prior to</span>
<span class="sd">           the forward pass.</span>
<span class="sd">        3. Setting PyTorch :class:`~torch.nn.Module` forward and backward hooks to check</span>
<span class="sd">           for NaNs in module outputs and gradients.</span>

<span class="sd">        Args:</span>
<span class="sd">            *args: Positional arguments to the forward function of the module defined in</span>
<span class="sd">                :meth:`_forward`.</span>
<span class="sd">            **kwargs: Keyword arguments to the forward function of the module defined in</span>
<span class="sd">                :meth:`_forward`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">(</span>
                    <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_args</span><span class="p">,</span>
                    <span class="n">input_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_input_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span>
                    <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_kwargs</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;Got exception when passing input size: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_args</span><span class="p">,</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_kwargs</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fns</span><span class="p">:</span>
                <span class="nb">super</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">signature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_verify_signature</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">dm</span><span class="o">.</span><span class="n">_check_nan</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">LazyModule</span><span class="o">.</span><span class="n">_check_nan_hook</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span><span class="n">LazyModule</span><span class="o">.</span><span class="n">_check_nan_hook</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="LazyModule.expect_signature"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.LazyModule.expect_signature">[docs]</a>    <span class="k">def</span> <span class="nf">expect_signature</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">signature</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set the expected module input / output signature.</span>

<span class="sd">        Note that this feature is currently not fully functional. More details will be</span>
<span class="sd">        added after implementation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">signature</span> <span class="o">=</span> <span class="n">signature</span></div>

    <span class="k">def</span> <span class="nf">_verify_signature</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1"># TODO: Implement this.</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_get_input_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_input_size</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="n">AttrTensor</span><span class="p">,</span> <span class="n">Variable</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_check_nan_hook</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">):</span>
        <span class="n">_utils</span><span class="o">.</span><span class="n">check_nan</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="NoMeta"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.NoMeta">[docs]</a><span class="k">class</span> <span class="nc">NoMeta</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A wrapper module to allow regular modules to take</span>
<span class="sd">    :class:`~deepmatcher.batch.AttrTensor` s as input.</span>

<span class="sd">    A forward pass through this module, will perform the following:</span>

<span class="sd">    * If the module input is an :class:`~deepmatcher.batch.AttrTensor`, gets the data from</span>
<span class="sd">      it, and use as input.</span>
<span class="sd">    * Perform a forward pass through wrapped module with the modified input.</span>
<span class="sd">    * Using metadata information from the module input (if provided), wrap the result into</span>
<span class="sd">      an :class:`~deepmatcher.batch.AttrTensor` and return it.</span>

<span class="sd">    Args:</span>
<span class="sd">        module (:class:`~torch.nn.Module`): The module to wrap.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NoMeta</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">module_args</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
            <span class="n">module_args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arg</span><span class="o">.</span><span class="n">data</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">AttrTensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">arg</span><span class="p">)</span>

        <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">module_args</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">AttrTensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">results</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">results</span> <span class="o">=</span> <span class="p">(</span><span class="n">results</span><span class="p">,)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;Number of inputs must equal number of outputs, or &#39;</span>
                    <span class="s1">&#39;number of inputs must be 1 or number of outputs must be 1.&#39;</span><span class="p">)</span>

            <span class="n">results_with_meta</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)):</span>
                <span class="n">arg_i</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">results_with_meta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">AttrTensor</span><span class="o">.</span><span class="n">from_old_metadata</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="n">arg_i</span><span class="p">]))</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">results_with_meta</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">results_with_meta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">results_with_meta</span><span class="p">)</span></div>


<div class="viewcode-block" id="ModuleMap"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.ModuleMap">[docs]</a><span class="k">class</span> <span class="nc">ModuleMap</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Holds submodules in a map.</span>

<span class="sd">    Similar to :class:`torch.nn.ModuleList`, but for maps.</span>

<span class="sd">    Example::</span>

<span class="sd">        import torch.nn as nn</span>
<span class="sd">        import deepmatcher as dm</span>

<span class="sd">        class MyModule(nn.Module):</span>
<span class="sd">            def __init__(self):</span>
<span class="sd">                super(MyModule, self).__init__()</span>
<span class="sd">                linears = dm.ModuleMap()</span>
<span class="sd">                linears[&#39;type&#39;] = nn.Linear(10, 10)</span>
<span class="sd">                linears[&#39;color&#39;] = nn.Linear(10, 10)</span>
<span class="sd">                self.linears = linears</span>

<span class="sd">            def forward(self, x1, x2):</span>
<span class="sd">                y1, y2 = self.linears[&#39;type&#39;], self.linears[&#39;color&#39;]</span>
<span class="sd">                return y1, y2</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">):</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__delitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="nb">delattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultiSequential"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.MultiSequential">[docs]</a><span class="k">class</span> <span class="nc">MultiSequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A sequential container that supports multiple module inputs and outputs.</span>

<span class="sd">    This is an extenstion of PyTorch&#39;s :class:`~torch.nn.Sequential` module that allows</span>
<span class="sd">    each module to have multiple inputs and / or outputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">modules</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">AttrTensor</span><span class="p">):</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span></div>


<div class="viewcode-block" id="LazyModuleFn"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.LazyModuleFn">[docs]</a><span class="k">class</span> <span class="nc">LazyModuleFn</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A Lazy Module which simply wraps the :class:`~torch.nn.Module` returned by a</span>
<span class="sd">    specified function.</span>

<span class="sd">    This provides a way to convert a PyTorch :class:`~torch.nn.Module` into a</span>
<span class="sd">    :class:`LazyModule`.</span>

<span class="sd">    Args:</span>
<span class="sd">        fn (callable):</span>
<span class="sd">            Function that returns a :class:`~torch.nn.Module`.</span>

<span class="sd">        *args:</span>
<span class="sd">            Positional arguments to the function `fn`.</span>

<span class="sd">        *kwargs:</span>
<span class="sd">            Keyword arguments to the function `fn`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="RNN"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.RNN">[docs]</a><span class="k">class</span> <span class="nc">RNN</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;__init__(unit_type=&#39;gru&#39;, hidden_size=None, layers=1, bidirectional=True, dropout=0, input_dropout=0, last_layer_dropout=0, bypass_network=None, connect_num_layers=1, input_size=None, **kwargs)</span>

<span class="sd">    A multi layered RNN that supports dropout and residual / highway connections.</span>

<span class="sd">    Args:</span>
<span class="sd">        unit_type (string):</span>
<span class="sd">            One of the support RNN unit types:</span>

<span class="sd">            * &#39;**gru**&#39;: Apply a gated recurrent unit (GRU) RNN. Uses PyTorch</span>
<span class="sd">              :class:`~torch.nn.GRU` under the hood.</span>
<span class="sd">            * &#39;**lstm**&#39;: Apply a long short-term memory unit (LSTM) RNN. Uses PyTorch</span>
<span class="sd">              :class:`~torch.nn.LSTM` under the hood.</span>
<span class="sd">            * &#39;**rnn**&#39;: Apply an Elman RNN. Uses PyTorch :class:`~torch.nn.RNN` under the</span>
<span class="sd">              hood.</span>

<span class="sd">        hidden_size (int):</span>
<span class="sd">            The hidden size of all RNN layers.</span>

<span class="sd">        layers (int):</span>
<span class="sd">            Number of RNN layers.</span>

<span class="sd">        bidirectional (bool):</span>
<span class="sd">            Whether to use bidirectional RNNs.</span>

<span class="sd">        dropout (float):</span>
<span class="sd">            If non-zero, applies dropout to the outputs of each RNN layer except the last</span>
<span class="sd">            layer. Dropout probability must be between 0 and 1.</span>

<span class="sd">        input_dropout (float):</span>
<span class="sd">            If non-zero, applies dropout to the input to this module. Dropout probability</span>
<span class="sd">            must be between 0 and 1.</span>

<span class="sd">        last_layer_dropout (float):</span>
<span class="sd">            If non-zero, applies dropout to the output of the last RNN layer. Dropout</span>
<span class="sd">            probability must be between 0 and 1.</span>

<span class="sd">        bypass_network (string or :class:`Bypass` or callable):</span>
<span class="sd">            The bypass network (e.g. residual or highway network) to apply every</span>
<span class="sd">            `connect_num_layers` layers. Argument must specify a :ref:`bypass-op`</span>
<span class="sd">            operation. If None, does not use a bypass network.</span>

<span class="sd">        connect_num_layers (int):</span>
<span class="sd">            The number of layers between each bypass operation. Note that the layers in</span>
<span class="sd">            which dropout is applied is also controlled by this. If `layers` is 6 and</span>
<span class="sd">            `connect_num_layers` is 2, then a bypass network is applied after the</span>
<span class="sd">            2nd, 4th and 6th layers. Further, if `dropout` is non-zero, it will only be</span>
<span class="sd">            applied after the 2nd and 4th layers.</span>

<span class="sd">        input_size (int):</span>
<span class="sd">            The number of features in the input to the module. This parameter will be</span>
<span class="sd">            automatically specified by :class:`LazyModule`.</span>

<span class="sd">        **kwargs (dict):</span>
<span class="sd">            Additional keyword arguments are passed to the underlying PyTorch RNN module.</span>

<span class="sd">    Input: One 3d tensor of shape `(batch, seq_len, input_size)`.</span>
<span class="sd">        The tensor should be wrapped within an :class:`~deepmatcher.batch.AttrTensor`</span>
<span class="sd">        which contains metadata about the batch.</span>
<span class="sd">    Output: One 3d tensor of shape `(batch, seq_len, output_size)`.</span>
<span class="sd">        This will be wrapped within an :class:`~deepmatcher.batch.AttrTensor` (with</span>
<span class="sd">        metadata information unchanged). `output_size` need not be the same as</span>
<span class="sd">        `input_size`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_supported_styles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;rnn&#39;</span><span class="p">,</span> <span class="s1">&#39;gru&#39;</span><span class="p">,</span> <span class="s1">&#39;lstm&#39;</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports_style</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">style</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_supported_styles</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">unit_type</span><span class="o">=</span><span class="s1">&#39;gru&#39;</span><span class="p">,</span>
              <span class="n">hidden_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
              <span class="n">input_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
              <span class="n">last_layer_dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
              <span class="n">bypass_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">connect_num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">input_size</span> <span class="k">if</span> <span class="n">hidden_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">hidden_size</span>
        <span class="n">last_layer_dropout</span> <span class="o">=</span> <span class="n">dropout</span> <span class="k">if</span> <span class="n">last_layer_dropout</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">last_layer_dropout</span>

        <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
            <span class="n">hidden_size</span> <span class="o">//=</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="n">bypass_network</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">layers</span> <span class="o">%</span> <span class="n">connect_num_layers</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="n">rnn_groups</span> <span class="o">=</span> <span class="n">layers</span> <span class="o">//</span> <span class="n">connect_num_layers</span>
            <span class="n">layers_per_group</span> <span class="o">=</span> <span class="n">connect_num_layers</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rnn_groups</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">layers_per_group</span> <span class="o">=</span> <span class="n">layers</span>

        <span class="n">bad_args</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s1">&#39;input_size&#39;</span><span class="p">,</span> <span class="s1">&#39;input_size&#39;</span><span class="p">,</span> <span class="s1">&#39;num_layers&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_first&#39;</span><span class="p">,</span> <span class="s1">&#39;dropout&#39;</span><span class="p">,</span>
            <span class="s1">&#39;bidirectional&#39;</span>
        <span class="p">]</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">([</span><span class="n">a</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">bad_args</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_groups</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropouts</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bypass_networks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dropout</span> <span class="o">=</span> <span class="n">NoMeta</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">input_dropout</span><span class="p">))</span>

        <span class="n">rnn_in_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rnn_groups</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rnn_groups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_get_rnn_module</span><span class="p">(</span>
                    <span class="n">unit_type</span><span class="p">,</span>
                    <span class="n">input_size</span><span class="o">=</span><span class="n">rnn_in_size</span><span class="p">,</span>
                    <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                    <span class="n">num_layers</span><span class="o">=</span><span class="n">layers_per_group</span><span class="p">,</span>
                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                    <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">g</span> <span class="o">!=</span> <span class="n">rnn_groups</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dropouts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dropouts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">last_layer_dropout</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bypass_networks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_bypass_module</span><span class="p">(</span><span class="n">bypass_network</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
                <span class="n">rnn_in_size</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">rnn_in_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_with_meta</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dropout</span><span class="p">(</span><span class="n">input_with_meta</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">rnn</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">bypass</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropouts</span><span class="p">,</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">bypass_networks</span><span class="p">):</span>
            <span class="n">new_output</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">rnn</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">bypass</span><span class="p">:</span>
                <span class="n">new_output</span> <span class="o">=</span> <span class="n">bypass</span><span class="p">(</span><span class="n">new_output</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">new_output</span>

        <span class="k">return</span> <span class="n">AttrTensor</span><span class="o">.</span><span class="n">from_old_metadata</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">input_with_meta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_rnn_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">unit_type</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">unit_type</span><span class="o">.</span><span class="n">upper</span><span class="p">())(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="AlignmentNetwork"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.AlignmentNetwork">[docs]</a><span class="k">class</span> <span class="nc">AlignmentNetwork</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;__init__(style=&#39;decomposable&#39;, hidden_size=None, transform_network=&#39;2-layer-highway&#39;, input_size=None)</span>

<span class="sd">    Neural network to compute alignment between two vector sequences.</span>

<span class="sd">    Takes two sequences of vectors, aligns the words in them, and returns</span>
<span class="sd">    the corresponding alignment matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        style (string): One of the following strings:</span>

<span class="sd">            * &#39;**decomposable**&#39;: Use decomposable attention. Alignment score between the</span>
<span class="sd">              :math:`i^{th}` vector in the first sequence :math:`a_i` , and the</span>
<span class="sd">              :math:`j^{th}` vector in the second sequence :math:`b_j` is computed as</span>
<span class="sd">              follows:</span>

<span class="sd">              .. math::</span>

<span class="sd">                  score(a_i, b_j) = F(a_i)^T F(b_j)</span>

<span class="sd">              where :math:`F` is a :ref:`transform-op` operation. Refer the</span>
<span class="sd">              `decomposable attention paper &lt;https://arxiv.org/abs/1606.01933&gt;`__ for more</span>
<span class="sd">              details.</span>

<span class="sd">            * &#39;**general**&#39;: Use general attention. Alignment score between the</span>
<span class="sd">              :math:`i^{th}` vector in the first sequence :math:`a_i` , and the</span>
<span class="sd">              :math:`j^{th}` vector in the second sequence :math:`b_j` is computed as</span>
<span class="sd">              follows:</span>

<span class="sd">              .. math::</span>

<span class="sd">                  score(a_i, b_j) = a_i^T F(b_j)</span>

<span class="sd">              where :math:`F` is a :ref:`transform-op` operation. Refer the `Luong attention</span>
<span class="sd">              paper &lt;https://arxiv.org/abs/1508.04025&gt;`__ for more details.</span>

<span class="sd">            * &#39;**dot**&#39;: Use dot product attention. Alignment score between the</span>
<span class="sd">              :math:`i^{th}` vector in the first sequence :math:`a_i` , and the</span>
<span class="sd">              :math:`j^{th}` vector in the second sequence :math:`b_j` is computed as</span>
<span class="sd">              follows:</span>

<span class="sd">            .. math::</span>

<span class="sd">                score(a_i, b_j) = a_i^T b_j</span>

<span class="sd">        hidden_size (int):</span>
<span class="sd">            The hidden size to use for the :ref:`transform-op` operation, if applicable</span>
<span class="sd">            for the specified `style`.</span>

<span class="sd">        transform_network (string or :class:`~deepmatcher.modules.Transform` or callable):</span>
<span class="sd">            The neural network to transform the input vectors, if applicable for the</span>
<span class="sd">            specified `style`. Argument must specify a :ref:`transform-op` operation.</span>

<span class="sd">        input_size (int):</span>
<span class="sd">            The number of features in the input to the module. This parameter will be</span>
<span class="sd">            automatically specified by :class:`LazyModule`.</span>

<span class="sd">    Input: Two 3d tensors.</span>
<span class="sd">        Two 3d tensors of shape `(batch, seq1_len, input_size)` and</span>
<span class="sd">        `(batch, seq2_len, input_size)`.</span>

<span class="sd">    Output: One 3d tensor of shape `(batch, seq1_len, seq2_len)`.</span>
<span class="sd">        The output represents the alignment matrix and contains unnormalized scores.</span>
<span class="sd">        `output_size` need not be the same as `input_size`, but all other dimensions will</span>
<span class="sd">        remain unchanged.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_supported_styles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;dot&#39;</span><span class="p">,</span> <span class="s1">&#39;general&#39;</span><span class="p">,</span> <span class="s1">&#39;decomposable&#39;</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports_style</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">style</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_supported_styles</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">style</span><span class="o">=</span><span class="s1">&#39;decomposable&#39;</span><span class="p">,</span>
              <span class="n">hidden_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">transform_network</span><span class="o">=</span><span class="s1">&#39;2-layer-highway&#39;</span><span class="p">,</span>
              <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">style</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;general&#39;</span><span class="p">,</span> <span class="s1">&#39;decomposable&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;general&#39;</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">hidden_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">hidden_size</span> <span class="o">==</span> <span class="n">input_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">_transform_module</span><span class="p">(</span><span class="n">transform_network</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="c1"># elif style in [&#39;concat&#39;, &#39;concat_dot&#39;]:</span>
        <span class="c1">#     self.input_transform = nn.ModuleList()</span>
        <span class="c1">#     self.input_transform.append(_transform_module(transform_network, hidden_size))</span>
        <span class="c1">#     if style == &#39;concat&#39;:</span>
        <span class="c1">#         self.input_transform.append(Transform(&#39;1-layer&#39;, output_size=1))</span>
        <span class="c1">#     self.context_transform = _transform_module(transform_network, hidden_size,</span>
        <span class="c1">#                                                output_size)</span>
        <span class="c1">#     if style == &#39;concat_dot&#39;:</span>
        <span class="c1">#         self.output_transform = Transform(</span>
        <span class="c1">#             &#39;1-layer&#39;, non_linearity=None, output_size=1)</span>
        <span class="k">elif</span> <span class="n">style</span> <span class="o">!=</span> <span class="s1">&#39;dot&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Unknown AlignmentNetwork style&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">=</span> <span class="n">style</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;dot&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
                <span class="nb">input</span><span class="p">,</span>  <span class="c1"># batch x len1 x input_size</span>
                <span class="n">context</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># batch x ch x input_size</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;general&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
                <span class="nb">input</span><span class="p">,</span>  <span class="c1"># batch x len1 x input_size</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">context</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># batch x input_size x len2</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;decomposable&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span>  <span class="c1"># batch x hidden_size x len2</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">context</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># batch x hidden_size x len2</span></div>
        <span class="c1"># elif self.style in [&#39;concat&#39;, &#39;concat_dot&#39;]:</span>
        <span class="c1">#     # batch x len1 x 1 x output_size</span>
        <span class="c1">#     input_transformed = self.input_transform(input).unsqueeze(2)</span>
        <span class="c1">#</span>
        <span class="c1">#     # batch x 1 x len2 x output_size</span>
        <span class="c1">#     context_transformed = self.context_transform(context).unsqueeze(1)</span>
        <span class="c1">#</span>
        <span class="c1">#     # batch x len1 x len2 x output_size</span>
        <span class="c1">#     pairwise_transformed = input_transformed + context_transformed</span>
        <span class="c1">#</span>
        <span class="c1">#     if self.style == &#39;concat&#39;:</span>
        <span class="c1">#         # batch x len1 x len2</span>
        <span class="c1">#         return pairwise_transformed.squeeze(3)</span>
        <span class="c1">#</span>
        <span class="c1">#     # batch x len1 x len2</span>
        <span class="c1">#     return self.output_transform(pairwise_transformed).squeeze(3)</span>


<div class="viewcode-block" id="Lambda"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.Lambda">[docs]</a><span class="k">class</span> <span class="nc">Lambda</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Wrapper to convert a function to a module.</span>

<span class="sd">    Args:</span>
<span class="sd">        lambd (callable): The function to convert into a module. It must take in one or</span>
<span class="sd">            more Pytorch :class:`~torch.Tensor` s and return one or more</span>
<span class="sd">            :class:`~torch.Tensor` s.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Lambda</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span></div>


<div class="viewcode-block" id="Pool"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.Pool">[docs]</a><span class="k">class</span> <span class="nc">Pool</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;__init__(style, alpha=0.001)</span>

<span class="sd">    Module that aggregates a given sequence of vectors to produce a single vector.</span>

<span class="sd">    Args:</span>
<span class="sd">        style (string): One of the following strings:</span>

<span class="sd">            * &#39;**avg**&#39;: Take the average of the input vectors. Given a sequence of</span>
<span class="sd">              vectors :math:`x_{1:N}` :</span>

<span class="sd">              .. math::</span>

<span class="sd">                  Pool(x_{1:N}) = \frac{1}{N} \sum_1^N x_i</span>

<span class="sd">            * &#39;**divsqrt**&#39;: Take the sum of the input vectors :math:`x_{1:N}` and divide</span>
<span class="sd">              by :math:`\sqrt{N}` :</span>

<span class="sd">              .. math::</span>

<span class="sd">                  Pool(x_{1:N}) = \frac{1}{\sqrt{N}} \sum_1^N x_i</span>

<span class="sd">            * &#39;**inv-freq-avg**&#39;: Take the smooth inverse frequency weighted sum of the</span>
<span class="sd">              :math:`N` input vectors and divide by :math:`\sqrt{N}`. This is similar to</span>
<span class="sd">              the &#39;sif&#39; style but does not perform principal component removal. Given a</span>
<span class="sd">              sequence of vectors :math:`x_{1:N}` corresponding to words :math:`w_{1:N}`:</span>

<span class="sd">              .. math::</span>

<span class="sd">                  Pool(x_{1:N}) = \frac{1}{\sqrt{N}}</span>
<span class="sd">                      \sum_1^N \frac{\alpha}{\alpha + P(w)} x_i</span>

<span class="sd">              where :math:`P(w)` is the unigram probability of word :math:`w` (computed</span>
<span class="sd">              over all values of this attribute over the entire training dataset) and</span>
<span class="sd">              :math:`\alpha` is a scalar (specified by the `alpha` parameter).</span>
<span class="sd">              :math:`P(w)` is computed in :class:`~deepmatcher.data.MatchingDataset`, in</span>
<span class="sd">              the :meth:`~deepmatcher.data.MatchingDataset.compute_metadata` method.</span>

<span class="sd">            * &#39;**sif**&#39;: Compute the</span>
<span class="sd">              `SIF encoding &lt;https://openreview.net/pdf?id=SyK00v5xx&gt;`__ of the input</span>
<span class="sd">              vectors. Takes the smooth inverse frequency weighted sum of the :math:`N`</span>
<span class="sd">              input vectors and divides it by :math:`\sqrt{N}`. Also removes the</span>
<span class="sd">              projection of the resulting vector along the first principal component of</span>
<span class="sd">              all word embeddings (corresponding to words in this attribute in the</span>
<span class="sd">              training set). Given a sequence of vectors :math:`x_{1:N}` corresponding to</span>
<span class="sd">              words :math:`w_{1:N}`:</span>

<span class="sd">              .. math::</span>

<span class="sd">                  v_x = \frac{1}{\sqrt{N}} \sum_1^N \frac{\alpha}{\alpha + P(w)} x_i</span>

<span class="sd">                  Pool(x_{1:N}) = v_x - u^T u v_x</span>

<span class="sd">              where :math:`u` is the first principal component as described earlier,</span>
<span class="sd">              :math:`P(w)` is the unigram probability of word :math:`w` (computed over all</span>
<span class="sd">              values of this attribute over the entire training dataset) and</span>
<span class="sd">              :math:`\alpha` is a scalar (specified by the `alpha` parameter). :math:`u`</span>
<span class="sd">              and :math:`P(w)` are computed in :class:`~deepmatcher.data.MatchingDataset`,</span>
<span class="sd">              in the :meth:`~deepmatcher.data.MatchingDataset.compute_metadata` method.</span>

<span class="sd">            * &#39;**max**&#39;: Take the max of the input vector sequence along each input</span>
<span class="sd">              feature. If length metadata for each item in the input batch is available,</span>
<span class="sd">              ignores the padding vectors beyond the sequence length of each item when</span>
<span class="sd">              computing the max.</span>

<span class="sd">            * &#39;**last**&#39;: Take the last vector in the input vector sequence. If length</span>
<span class="sd">              metadata for each item in the input batch is available, ignores the padding</span>
<span class="sd">              vectors beyond the sequence length of each item when taking the last vector.</span>

<span class="sd">            * &#39;**last-simple**&#39;: Take the last vector in the input vector sequence. Does</span>
<span class="sd">              NOT take length metadata into account - simply takes the last vector for</span>
<span class="sd">              each input sequence in the batch.</span>

<span class="sd">            * &#39;**birnn-last**&#39;: Treats the input sequence as the output from a</span>
<span class="sd">              bidirectional RNN and takes the last outputs from the forward and backward</span>
<span class="sd">              RNNs. The first half of each vector is assumed to be from the forward RNN</span>
<span class="sd">              and the second half is assumed to be from the bakward RNN. The output thus</span>
<span class="sd">              is the concatenation of first half of the last vector in the input sequence</span>
<span class="sd">              and the last half of the first vector in the sequence. If length</span>
<span class="sd">              metadata for each item in the input batch is available, ignores the padding</span>
<span class="sd">              vectors beyond the sequence length of each item when taking the last vectors</span>
<span class="sd">              for the forward RNN.</span>

<span class="sd">            * &#39;**birnn-last-simple**&#39;: Treats the input sequence as the output from a</span>
<span class="sd">              bidirectional RNN and takes the last outputs from the forward and backward</span>
<span class="sd">              RNNs. Same as the &#39;birnn-last&#39; style but does not consider length metadata</span>
<span class="sd">              even if available.</span>

<span class="sd">        alpha (float): The value used to smooth the inverse word frequencies. Used for</span>
<span class="sd">            &#39;inv-freq-avg&#39; and &#39;sif&#39; `styles`.</span>


<span class="sd">    Input: A 3d tensor of shape `(batch, seq_len, input_size)`.</span>
<span class="sd">        The tensor should be wrapped within an :class:`~deepmatcher.batch.AttrTensor`</span>
<span class="sd">        which contains metadata about the batch.</span>

<span class="sd">    Output: A 2d tensor of shape `(batch, output_size)`.</span>
<span class="sd">        This will be wrapped within an :class:`~deepmatcher.batch.AttrTensor` (with</span>
<span class="sd">        metadata information unchanged). `output_size` need not be the same as</span>
<span class="sd">        `input_size`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_supported_styles</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;avg&#39;</span><span class="p">,</span> <span class="s1">&#39;divsqrt&#39;</span><span class="p">,</span> <span class="s1">&#39;inv-freq-avg&#39;</span><span class="p">,</span> <span class="s1">&#39;sif&#39;</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="s1">&#39;last&#39;</span><span class="p">,</span> <span class="s1">&#39;last-simple&#39;</span><span class="p">,</span>
        <span class="s1">&#39;birnn-last&#39;</span><span class="p">,</span> <span class="s1">&#39;birnn-last-simple&#39;</span>
    <span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports_style</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">style</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_supported_styles</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_style</span><span class="p">(</span><span class="n">style</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">=</span> <span class="n">style</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">alpha</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_with_meta</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">input_with_meta</span><span class="o">.</span><span class="n">data</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;last&#39;</span><span class="p">:</span>
            <span class="n">lengths</span> <span class="o">=</span> <span class="n">input_with_meta</span><span class="o">.</span><span class="n">lengths</span>
            <span class="n">lasts</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">lengths</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">lasts</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;last-simple&#39;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">:]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;birnn-last&#39;</span><span class="p">:</span>
            <span class="n">hsize</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">lengths</span> <span class="o">=</span> <span class="n">input_with_meta</span><span class="o">.</span><span class="n">lengths</span>
            <span class="n">lasts</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">lengths</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hsize</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>

            <span class="n">forward_outputs</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">forward_last</span> <span class="o">=</span> <span class="n">forward_outputs</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">lasts</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">backward_last</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">hsize</span><span class="p">:]</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">forward_last</span><span class="p">,</span> <span class="n">backward_last</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;birnn-last-simple&#39;</span><span class="p">:</span>
            <span class="n">forward_last</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">:</span><span class="n">hsize</span><span class="p">]</span>
            <span class="n">backward_last</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">hsize</span><span class="p">:]</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">forward_last</span><span class="p">,</span> <span class="n">backward_last</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_with_meta</span><span class="o">.</span><span class="n">lengths</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">sequence_mask</span><span class="p">(</span><span class="n">input_with_meta</span><span class="o">.</span><span class="n">lengths</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Make it broadcastable.</span>
                <span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_with_meta</span><span class="o">.</span><span class="n">lengths</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">sequence_mask</span><span class="p">(</span><span class="n">input_with_meta</span><span class="o">.</span><span class="n">lengths</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Make it broadcastable.</span>
                <span class="nb">input</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

            <span class="n">lengths</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">input_with_meta</span><span class="o">.</span><span class="n">lengths</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;avg&#39;</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">lengths</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;divsqrt&#39;</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;inv-freq-avg&#39;</span><span class="p">:</span>
                <span class="n">inv_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">/</span> <span class="p">(</span><span class="n">input_with_meta</span><span class="o">.</span><span class="n">word_probs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
                <span class="n">weighted</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">Variable</span><span class="p">(</span><span class="n">inv_probs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">weighted</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;sif&#39;</span><span class="p">:</span>
                <span class="n">inv_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">/</span> <span class="p">(</span><span class="n">input_with_meta</span><span class="o">.</span><span class="n">word_probs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
                <span class="n">weighted</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">Variable</span><span class="p">(</span><span class="n">inv_probs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
                <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">weighted</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sqrt</span><span class="p">())</span>
                <span class="n">pc</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">input_with_meta</span><span class="o">.</span><span class="n">pc</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">proj_v_on_pc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">pc</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">pc</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">v</span> <span class="o">-</span> <span class="n">proj_v_on_pc</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">+</span> <span class="s1">&#39; is not implemented.&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">AttrTensor</span><span class="o">.</span><span class="n">from_old_metadata</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">input_with_meta</span><span class="p">)</span></div>


<div class="viewcode-block" id="Merge"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.Merge">[docs]</a><span class="k">class</span> <span class="nc">Merge</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;__init__(style)</span>

<span class="sd">    Module that takes two or more vectors and merges them produce a single vector.</span>

<span class="sd">    Args:</span>
<span class="sd">        style (string): One of the following strings:</span>

<span class="sd">            * &#39;**concat**&#39;: Concatenate all the input vectors along the last dimension</span>
<span class="sd">              (-1).</span>
<span class="sd">            * &#39;**diff**&#39;: Take the difference between two input vectors.</span>
<span class="sd">            * &#39;**abs-diff**&#39;: Take the absolute difference between two input vectors.</span>
<span class="sd">            * &#39;**concat-diff**&#39;: Concatenate the two input vectors, take the difference</span>
<span class="sd">              between the two vectors, and concatenate these two resulting vectors.</span>
<span class="sd">            * &#39;**concat-abs-diff**&#39;: Concatenate the two input vectors, take the absolute</span>
<span class="sd">              difference between the two vectors, and concatenate these two resulting</span>
<span class="sd">              vectors.</span>
<span class="sd">            * &#39;**mul**&#39;: Take the element-wise multiplication between the two input</span>
<span class="sd">              vectors.</span>

<span class="sd">    Input: N K-d tensors of shape `(D1, D2, ..., input_size)`.</span>
<span class="sd">        N and K are both 2 or more.</span>

<span class="sd">    Output: One K-d tensor of shape `(D1, D2, ..., output_size)`.</span>
<span class="sd">        `output_size` need not be the same as `input_size`, but all other dimensions will</span>
<span class="sd">        remain unchanged.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_style_map</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;concat&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;diff&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span>
        <span class="s1">&#39;abs-diff&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">),</span>
        <span class="s1">&#39;concat-diff&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;concat-abs-diff&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)),</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;mul&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports_style</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">style</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_style_map</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_style</span><span class="p">(</span><span class="n">style</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">Merge</span><span class="o">.</span><span class="n">_style_map</span><span class="p">[</span><span class="n">style</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span></div>


<div class="viewcode-block" id="Bypass"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.Bypass">[docs]</a><span class="k">class</span> <span class="nc">Bypass</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;__init__(style)</span>

<span class="sd">    Module that helps bypass a given transformation of an input.</span>

<span class="sd">    Supports residual and highway styles of bypass.</span>

<span class="sd">    Args:</span>
<span class="sd">        style (string): One of the following strings:</span>

<span class="sd">            * &#39;**residual**&#39;: Uses a `residual network &lt;https://arxiv.org/abs/1512.03385&gt;`__.</span>
<span class="sd">            * &#39;**highway**&#39;: Uses a `highway network &lt;https://arxiv.org/abs/1505.00387&gt;`__.</span>

<span class="sd">    Input: Two N-d tensors.</span>
<span class="sd">        Two N-d tensors of shape `(D1, D2, ..., transformed_size)` and</span>
<span class="sd">        `(D1, D2, ..., input_size)`. The first tensor should corresponds to the</span>
<span class="sd">        transformed version of the second input.</span>

<span class="sd">    Output: One N-d tensor of shape `(D1, D2, ..., transformed_size)`.</span>
<span class="sd">        Note that the shape of the output will match the shape of the first input tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_supported_styles</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;residual&#39;</span><span class="p">,</span> <span class="s1">&#39;highway&#39;</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports_style</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">style</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">style</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_supported_styles</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">style</span><span class="p">,</span> <span class="n">residual_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">highway_bias</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_style</span><span class="p">(</span><span class="n">style</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">=</span> <span class="n">style</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">residual_scale</span> <span class="o">=</span> <span class="n">residual_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">highway_bias</span> <span class="o">=</span> <span class="n">highway_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">highway_gate</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transformed</span><span class="p">,</span> <span class="n">raw</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">transformed</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">raw</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">tsize</span> <span class="o">=</span> <span class="n">transformed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">rsize</span> <span class="o">=</span> <span class="n">raw</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">adjusted_raw</span> <span class="o">=</span> <span class="n">raw</span>
        <span class="k">if</span> <span class="n">tsize</span> <span class="o">&lt;</span> <span class="n">rsize</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">rsize</span> <span class="o">/</span> <span class="n">tsize</span> <span class="o">&lt;=</span> <span class="mi">50</span>
            <span class="k">if</span> <span class="n">rsize</span> <span class="o">%</span> <span class="n">tsize</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">padded</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tsize</span> <span class="o">-</span> <span class="n">rsize</span> <span class="o">%</span> <span class="n">tsize</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">padded</span> <span class="o">=</span> <span class="n">raw</span>
            <span class="n">adjusted_raw</span> <span class="o">=</span> <span class="n">padded</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">raw</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">tsize</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                <span class="n">tsize</span> <span class="o">/</span> <span class="n">rsize</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">tsize</span> <span class="o">&gt;</span> <span class="n">rsize</span><span class="p">:</span>
            <span class="n">multiples</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">tsize</span> <span class="o">/</span> <span class="n">rsize</span><span class="p">)</span>
            <span class="n">adjusted_raw</span> <span class="o">=</span> <span class="n">raw</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">raw</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">multiples</span><span class="p">)</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tsize</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;residual&#39;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">transformed</span> <span class="o">+</span> <span class="n">adjusted_raw</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_scale</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">*=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">res</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">style</span> <span class="o">==</span> <span class="s1">&#39;highway&#39;</span><span class="p">:</span>
            <span class="n">transform_gate</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">highway_gate</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">highway_bias</span><span class="p">)</span>
            <span class="n">carry_gate</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">transform_gate</span>
            <span class="k">return</span> <span class="n">transform_gate</span> <span class="o">*</span> <span class="n">transformed</span> <span class="o">+</span> <span class="n">carry_gate</span> <span class="o">*</span> <span class="n">adjusted_raw</span></div>


<div class="viewcode-block" id="Transform"><a class="viewcode-back" href="../../../modules.html#deepmatcher.modules.Transform">[docs]</a><span class="k">class</span> <span class="nc">Transform</span><span class="p">(</span><span class="n">LazyModule</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;__init__(style, layers=1, bypass_network=None, non_linearity=&#39;leaky_relu&#39;, hidden_size=None, output_size=None, input_size=None)</span>

<span class="sd">    A multi layered transformation module.</span>

<span class="sd">    Supports various non-linearities and bypass operations.</span>

<span class="sd">    Args:</span>
<span class="sd">        style (string):</span>
<span class="sd">            A string containing one or more of the following 3 parts, separated by dashes</span>
<span class="sd">            (-):</span>

<span class="sd">            * &#39;**&lt;N&gt;-layer**&#39;: Specifies the number of layers. &lt;N&gt; sets the `layers`</span>
<span class="sd">              parameter. E.g.: &#39;**2-layer**-highway&#39;.</span>

<span class="sd">            * &#39;**&lt;nonlinearity&gt;**&#39;: Specifies the non-linearity used after each layer.</span>
<span class="sd">              Sets the `non_linearity` parameter, refer that for details.</span>

<span class="sd">            * &#39;**&lt;bypass&gt;**&#39; Specifies the :ref:`bypass-op` operation to use.</span>
<span class="sd">              Sets the `bypass_network` parameter. &lt;bypass&gt; is one of:</span>

<span class="sd">              * &#39;residual&#39;: Use :class:`Bypass` with &#39;residual&#39; `style`.</span>
<span class="sd">              * &#39;highway&#39;: Use :class:`Bypass` with &#39;highway&#39; `style`.</span>

<span class="sd">            If any of the 3 parts are missing, the default value for the corresponding</span>
<span class="sd">            parameter is used.</span>

<span class="sd">            Examples: Sample `styles`</span>
<span class="sd">                &#39;3-layer-relu-highway&#39;, &#39;tanh-residual-2-layer&#39;, &#39;tanh&#39;, &#39;highway&#39;,</span>
<span class="sd">                &#39;4-layer&#39;.</span>

<span class="sd">        layers (int):</span>
<span class="sd">            Number of linear transformation layers to use.</span>

<span class="sd">        bypass_network (string or :class:`Bypass` or callable):</span>
<span class="sd">            The bypass network (e.g. residual or highway network) to apply every layer.</span>
<span class="sd">            The input to each linear layer is considered as the raw input to the bypass</span>
<span class="sd">            network and the output of the non-linearity operation is considered as the</span>
<span class="sd">            transformed input. Argument must specify a :ref:`bypass-op` operation. If</span>
<span class="sd">            None, does not use a bypass network.</span>

<span class="sd">        non_linearity (string):</span>
<span class="sd">            The non-linearity to use after each linear layer. One of:</span>

<span class="sd">            * &#39;**leaky_relu**&#39;: Use PyTorch :class:`~torch.nn.LeakyReLU`.</span>
<span class="sd">            * &#39;**relu**&#39;: Use PyTorch :class:`~torch.nn.ReLU`.</span>
<span class="sd">            * &#39;**elu**&#39;: Use PyTorch :class:`~torch.nn.ELU`.</span>
<span class="sd">            * &#39;**selu**&#39;: Use PyTorch :class:`~torch.nn.SELU`.</span>
<span class="sd">            * &#39;**glu**&#39;: Use PyTorch :func:`~torch.nn.functional.glu`.</span>
<span class="sd">            * &#39;**tanh**&#39;: Use PyTorch :class:`~torch.nn.Tanh`.</span>
<span class="sd">            * &#39;**sigmoid**&#39;: Use PyTorch :class:`~torch.nn.Sigmoid`.</span>

<span class="sd">        hidden_size (int):</span>
<span class="sd">            The hidden size of the linear transformation layers. If None, will be set</span>
<span class="sd">            to be equal to `input_size`.</span>

<span class="sd">        output_size (int):</span>
<span class="sd">            The hidden size of the last linear transformation layer. Will determine the</span>
<span class="sd">            number of features in the output of the module. If None, will be set to be</span>
<span class="sd">            equal to the `hidden_size`.</span>

<span class="sd">        input_size (int):</span>
<span class="sd">            The number of features in the input to the module. This parameter will be</span>
<span class="sd">            automatically specified by :class:`LazyModule`.</span>

<span class="sd">    Input: An N-d tensor of shape `(D1, D2, ..., input_size)`.</span>
<span class="sd">        N is 2 or more.</span>

<span class="sd">    Output: An N-d tensor of shape `(D1, D2, ..., output_size)`.</span>
<span class="sd">        `output_size` need not be the same as `input_size`, but all other dimensions will</span>
<span class="sd">        remain unchanged.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_supported_nonlinearities</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="s1">&#39;elu&#39;</span><span class="p">,</span> <span class="s1">&#39;selu&#39;</span><span class="p">,</span> <span class="s1">&#39;glu&#39;</span><span class="p">,</span> <span class="s1">&#39;leaky_relu&#39;</span>
    <span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">supports_nonlinearity</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">nonlin</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">nonlin</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_supported_nonlinearities</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
              <span class="n">style</span><span class="p">,</span>
              <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">bypass_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">non_linearity</span><span class="o">=</span><span class="s1">&#39;leaky_relu&#39;</span><span class="p">,</span>
              <span class="n">hidden_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="ow">or</span> <span class="n">input_size</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span> <span class="ow">or</span> <span class="n">hidden_size</span>

        <span class="n">parts</span> <span class="o">=</span> <span class="n">style</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;layer&#39;</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">parts</span><span class="p">[</span><span class="n">parts</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s1">&#39;layer&#39;</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">part</span> <span class="ow">in</span> <span class="n">parts</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">Bypass</span><span class="o">.</span><span class="n">supports_style</span><span class="p">(</span><span class="n">part</span><span class="p">):</span>
                <span class="n">bypass_network</span> <span class="o">=</span> <span class="n">part</span>
            <span class="k">if</span> <span class="n">Transform</span><span class="o">.</span><span class="n">supports_nonlinearity</span><span class="p">(</span><span class="n">part</span><span class="p">):</span>
                <span class="n">non_linearity</span> <span class="o">=</span> <span class="n">part</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bypass_networks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>

        <span class="k">assert</span> <span class="p">(</span><span class="n">non_linearity</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">supports_nonlinearity</span><span class="p">(</span><span class="n">non_linearity</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">non_linearity</span> <span class="o">=</span> <span class="n">non_linearity</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">if</span> <span class="n">non_linearity</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="n">transform_in_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="n">transform_out_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">layer</span> <span class="o">==</span> <span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">transform_out_size</span> <span class="o">=</span> <span class="n">output_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">transform_in_size</span><span class="p">,</span> <span class="n">transform_out_size</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bypass_networks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_bypass_module</span><span class="p">(</span><span class="n">bypass_network</span><span class="p">))</span>
            <span class="n">transform_in_size</span> <span class="o">=</span> <span class="n">transform_out_size</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span>

        <span class="k">for</span> <span class="n">transform</span><span class="p">,</span> <span class="n">bypass</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bypass_networks</span><span class="p">):</span>
            <span class="n">new_output</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_linearity</span><span class="p">:</span>
                <span class="n">new_output</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_linearity</span><span class="p">)(</span><span class="n">new_output</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">bypass</span><span class="p">:</span>
                <span class="n">new_output</span> <span class="o">=</span> <span class="n">bypass</span><span class="p">(</span><span class="n">new_output</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">new_output</span>

        <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">_merge_module</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">get_module</span><span class="p">(</span><span class="n">Merge</span><span class="p">,</span> <span class="n">op</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">module</span><span class="p">:</span>
        <span class="n">module</span><span class="o">.</span><span class="n">expect_signature</span><span class="p">(</span><span class="s1">&#39;[AxB, AxB] -&gt; [AxC]&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span> <span class="nf">_bypass_module</span><span class="p">(</span><span class="n">op</span><span class="p">):</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">get_module</span><span class="p">(</span><span class="n">Bypass</span><span class="p">,</span> <span class="n">op</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">module</span><span class="p">:</span>
        <span class="n">module</span><span class="o">.</span><span class="n">expect_signature</span><span class="p">(</span><span class="s1">&#39;[AxB, AxC] -&gt; [AxB]&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span> <span class="nf">_transform_module</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span> <span class="ow">or</span> <span class="n">hidden_size</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">get_module</span><span class="p">(</span>
        <span class="n">Transform</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">module</span><span class="p">:</span>
        <span class="n">module</span><span class="o">.</span><span class="n">expect_signature</span><span class="p">(</span><span class="s1">&#39;[AxB] -&gt; [AxC]&#39;</span><span class="p">)</span>
        <span class="n">module</span><span class="o">.</span><span class="n">expect_signature</span><span class="p">(</span><span class="s1">&#39;[AxBxC] -&gt; [AxBxD]&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>


<span class="k">def</span> <span class="nf">_alignment_module</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
    <span class="n">module</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">get_module</span><span class="p">(</span>
        <span class="n">AlignmentNetwork</span><span class="p">,</span> <span class="n">op</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">module</span><span class="o">.</span><span class="n">expect_signature</span><span class="p">(</span><span class="s1">&#39;[AxBxC, AxDxC] -&gt; [AxBxD]&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">module</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Sidharth Mudgal, Han Li.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'0.1.0rc1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>